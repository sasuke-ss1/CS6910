{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshaping the data matrices\n",
    "X = X.reshape(X.shape[0], 784)\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)\n",
    "\n",
    "# Normalizing the pixel intensities\n",
    "X = X/255.0\n",
    "X_test = X_test/255.0\n",
    "\n",
    "# Split the X_train into a training set and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the training set = 54000\n",
      "Number of images in the validation set = 6000\n",
      "Number of images in the test set = 10000\n",
      "Number of classes = 10\n"
     ]
    }
   ],
   "source": [
    "# Number of training examples\n",
    "M = X_train.shape[0]\n",
    "\n",
    "# Number of validation samples\n",
    "Mval = X_val.shape[0]\n",
    "\n",
    "# Number of test examples\n",
    "Mtest = X_test.shape[0]\n",
    "\n",
    "# Number of features in the dataset\n",
    "num_features = 784\n",
    "\n",
    "# Number of classes\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# One hot encoding for class labels\n",
    "y_train_one_hot = np.zeros((10, M))\n",
    "y_train_one_hot[y_train, np.array(list(range(M)))] = 1\n",
    "\n",
    "y_val_one_hot = np.zeros((10, Mval))\n",
    "y_val_one_hot[y_val, np.array(list(range(Mval)))] = 1\n",
    "\n",
    "y_test_one_hot = np.zeros((10, Mtest))\n",
    "y_test_one_hot[y_test, np.array(list(range(Mtest)))] = 1\n",
    "\n",
    "print(\"Number of images in the training set =\", M)\n",
    "print(\"Number of images in the validation set =\", Mval)\n",
    "print(\"Number of images in the test set =\", Mtest)\n",
    "print(\"Number of classes =\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 54000)\n"
     ]
    }
   ],
   "source": [
    "# Modify shapes of the data matrices\n",
    "X_train = X_train.T\n",
    "X_val = X_val.T\n",
    "X_test = X_test.T\n",
    "X = X.T\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes = num_features\n",
    "output_nodes = num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1.+np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "def Relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def Relu_derivative(x):\n",
    "    return 1*(x>0) \n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - (np.tanh(x)**2))\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def softmax_derivative(x):\n",
    "    return softmax(x) * (1-softmax(x))\n",
    "\n",
    "def compute_multiclass_loss(Y, Y_hat, batch_size, loss, lamb, parameters):\n",
    "\n",
    "    if loss == 'categorical_crossentropy':\n",
    "         L = (-1.0 * np.sum(np.multiply(Y, np.log(Y_hat))))/batch_size\n",
    "    elif loss == 'mse':\n",
    "         L = (1/2) * np.sum((Y-Y_hat)**2)/batch_size\n",
    "\n",
    "    #Add L2 regularisation\n",
    "    acc = 0\n",
    "    for i in range(1, len(parameters)//2 + 1):\n",
    "        acc += np.sum(parameters[\"W\"+str(i)]**2)\n",
    "\n",
    "    L = L + (lamb/(2*batch_size))*acc\n",
    "\n",
    "    return L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims, init_mode=\"xavier\"):\n",
    "    '''Function to initialise weights, biases and velocities/previous updates of the NN\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    layer_dims: list\n",
    "        list of number of neurons per layer specifying layer dimensions in the format [#input_features,#hiddenunits...#hiddenunits,#outputclasses]\n",
    "\n",
    "    init_mode: string\n",
    "        initialisation mode, default-\"xavier\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    params: dict\n",
    "        contains weights and biases. eg params[W1] is weight for layer 1\n",
    "\n",
    "    previous updates: dict\n",
    "        previous updates initialisation. This is used for different perposes for different optimisers.\n",
    "\n",
    "    '''\n",
    "    np.random.seed(42)\n",
    "    params = {}\n",
    "    previous_updates = {}\n",
    "\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        if init_mode == 'random_normal':\n",
    "            params[\"W\"+str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1]) * 0.01\n",
    "        elif init_mode == 'random_uniform':\n",
    "            params[\"W\"+str(i)] = np.random.rand(layer_dims[i], layer_dims[i-1]) * 0.01\n",
    "        elif init_mode == 'xavier':\n",
    "            params[\"W\"+str(i)]= np.random.randn(layer_dims[i],layer_dims[i-1])*np.sqrt(2/(layer_dims[i]+layer_dims[i-1]))\n",
    "            \n",
    "        params[\"b\"+str(i)] = np.zeros((layer_dims[i], 1))\n",
    "        \n",
    "        previous_updates[\"W\"+str(i)] = np.zeros((layer_dims[i], layer_dims[i-1]))\n",
    "        previous_updates[\"b\"+str(i)] = np.zeros((layer_dims[i], 1))\n",
    "\n",
    "    return params,previous_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(X, params, activation_f):\n",
    "    '''Function to forward propagate a minibatch of data once through the NN\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: numpy array\n",
    "        data in (features,batch_size) format\n",
    "\n",
    "    params: dict\n",
    "        W and b of the NN\n",
    "\n",
    "    activation_f: string\n",
    "        activation function to be used except the output layer\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output: numpy array\n",
    "        contains the output probabilities for each class and each data sample after 1 pass\n",
    "    A: numpy array\n",
    "        contains all post-activations\n",
    "    Z: numpy array\n",
    "        contsins all pre-activations\n",
    "\n",
    "    '''\n",
    "    L = len(params)//2 + 1\n",
    "    A = [None]*L # activations\n",
    "    Z = [None]*L # pre-activations\n",
    "    \n",
    "    A[0] = X\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        W = params[\"W\"+str(l)]\n",
    "        b = params[\"b\"+str(l)]\n",
    "        \n",
    "        Z[l] = np.matmul(W,A[l-1]) + b \n",
    "        \n",
    "        if l == L-1:\n",
    "            A[l] = softmax(Z[l]) # activation function for output layer\n",
    "        else:\n",
    "            if activation_f == 'sigmoid':\n",
    "                A[l] = sigmoid(Z[l])\n",
    "            elif activation_f == 'relu':\n",
    "                A[l] = Relu(Z[l])\n",
    "            elif activation_f == 'tanh':\n",
    "                A[l] = tanh(Z[l])\n",
    "                \n",
    "    output = A[L-1]\n",
    "\n",
    "    return output,A,Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(y_hat, y,A, Z, params, activation_f, batch_size, loss, lamb):\n",
    "    '''Function to calculate gradients for a minibatch of data once through the NN through backpropagation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_hat: numpy array\n",
    "        output from forward propagation/ class probabilities\n",
    "\n",
    "    y: numpy array\n",
    "        actual class labels\n",
    "     \n",
    "    A: numpy array\n",
    "        post-activations\n",
    "\n",
    "    Z: numpy array\n",
    "        pre-activations   \n",
    "\n",
    "    params: dict\n",
    "        contains W and b on the NN   \n",
    "\n",
    "    activation_f: string\n",
    "        activation function to be used except the output layer\n",
    "\n",
    "    batch_size: int\n",
    "        mini-batch-size\n",
    "\n",
    "    loss: string\n",
    "        loss function (MSE/Categorical crossentropy)\n",
    "\n",
    "    lamb: float\n",
    "        L2 regularisation lambda\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gradients: dict\n",
    "        gradients wrt weights and biases\n",
    "\n",
    "    '''\n",
    "    L = len(params)//2 #no. of layers\n",
    "    gradients = {}\n",
    "\n",
    "        \n",
    "    #process last layer which has softmax\n",
    "    if loss == 'categorical_crossentropy':\n",
    "        gradients[\"dZ\"+str(L)] = A[L]-y\n",
    "    elif loss == 'mse':\n",
    "        gradients[\"dZ\"+str(L)] = (A[L]-y) * softmax_derivative(Z[L])\n",
    "    \n",
    "    #process other layers\n",
    "    for l in range(L,0,-1):\n",
    "        gradients[\"dW\" + str(l)] = (np.dot(gradients[\"dZ\" + str(l)], A[l-1].T) + lamb*params[\"W\"+str(l)]) / batch_size\n",
    "        gradients[\"db\" + str(l)] = np.sum(gradients[\"dZ\" + str(l)], axis=1, keepdims=True) / batch_size\n",
    "        \n",
    "        if l>1: \n",
    "            if activation_f == 'sigmoid':\n",
    "                gradients[\"dZ\"+str(l-1)] = np.matmul(params[\"W\" + str(l)].T, gradients[\"dZ\" + str(l)]) * sigmoid_derivative(Z[l-1])\n",
    "            elif activation_f == 'relu':\n",
    "                gradients[\"dZ\"+str(l-1)] = np.matmul(params[\"W\" + str(l)].T, gradients[\"dZ\" + str(l)]) * Relu_derivative(Z[l-1])\n",
    "            elif activation_f == 'tanh':\n",
    "                gradients[\"dZ\"+str(l-1)] = np.matmul(params[\"W\" + str(l)].T, gradients[\"dZ\" + str(l)]) * tanh_derivative(Z[l-1])\n",
    "        \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params_sgd(parameters,grads,learning_rate):\n",
    "    ''' Update W and b of the NN according to sgd updates\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    parameters: dict\n",
    "        contains weights and biases of the NN\n",
    "\n",
    "    grads: dict\n",
    "        contains gradients wrt W and b returned by backpropagation\n",
    "\n",
    "    learning_rate: float\n",
    "        learning rate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    parameters: dict\n",
    "        updated NN parameters\n",
    "\n",
    "    '''\n",
    "    L = len(parameters) // 2 \n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def update_parameters_momentum(parameters, grads, learning_rate, beta, previous_updates):\n",
    "    ''' Update W and b of the NN according to momentum updates\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    parameters: dict\n",
    "        contains weights and biases of the NN\n",
    "\n",
    "    grads: dict\n",
    "        contains gradients wrt W and b returned by backpropagation\n",
    "\n",
    "    learning_rate: float\n",
    "        learning rate\n",
    "    \n",
    "    beta: float\n",
    "        decay rate\n",
    "\n",
    "    previous_updates: dict\n",
    "        contains previous W and b values, accumulated in a weighted fashion along with the gradients eg.\n",
    "        previous_updates[Wi] = beta*previous_updates[Wi] + (1-beta)*gradient[dWi]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    parameters: dict\n",
    "        updated NN parameters\n",
    "        previous updates: dict\n",
    "        updated previous updates \n",
    "\n",
    "    '''\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(1, L + 1):\n",
    "        previous_updates[\"W\"+str(l)] = beta*previous_updates[\"W\"+str(l)] + (1-beta)*grads[\"dW\" + str(l)]\n",
    "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate*previous_updates[\"W\"+str(l)]\n",
    "        \n",
    "        previous_updates[\"b\"+str(l)] = beta*previous_updates[\"b\"+str(l)] + (1-beta)*grads[\"db\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate*previous_updates[\"b\"+str(l)]\n",
    "\n",
    "    return parameters, previous_updates\n",
    "\n",
    "def update_parameters_RMSprop(parameters, grads, learning_rate, beta, v):\n",
    "    ''' Update W and b of the NN according to RMSprop updates\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    parameters: dict\n",
    "        contains weights and biases of the NN\n",
    "\n",
    "    grads: dict\n",
    "        contains gradients wrt W and b returned by backpropagation\n",
    "\n",
    "    learning_rate: float\n",
    "        learning rate\n",
    "    \n",
    "    beta: float\n",
    "        decay rate\n",
    "\n",
    "    v: dict\n",
    "        contains previous W and b values, accumulated in a weighted fashion along with the gradients square eg.\n",
    "        v[Wi] = beta*v[Wi] + (1-beta)*(gradient[dWi]^2)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    parameters: dict\n",
    "        updated NN parameters\n",
    "\n",
    "    v: dict\n",
    "        updated \"velocities\"\n",
    "\n",
    "    '''\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    delta = 1e-6 # for numerical stability\n",
    "\n",
    "    for l in range(1, L + 1):\n",
    "        vdw = beta*v[\"W\" + str(l)] + (1-beta)*np.multiply(grads[\"dW\" + str(l)],grads[\"dW\" + str(l)])\n",
    "        vdb = beta*v[\"b\" + str(l)] + (1-beta)*np.multiply(grads[\"db\" + str(l)],grads[\"db\" + str(l)])\n",
    "\n",
    "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)] / (np.sqrt(vdw)+delta)\n",
    "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)] / (np.sqrt(vdb)+delta)\n",
    "\n",
    "        v[\"W\" + str(l)] = vdw\n",
    "        v[\"b\" + str(l)] = vdb\n",
    "\n",
    "    return parameters,v\n",
    "\n",
    "def update_parameters_adam(parameters, grads, learning_rate, v, m, t):\n",
    "    ''' Update W and b of the NN according to adam updates\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    parameters: dict\n",
    "        contains weights and biases of the NN\n",
    "\n",
    "    grads: dict\n",
    "        contains gradients wrt W and b returned by backpropagation\n",
    "\n",
    "    learning_rate: float\n",
    "        learning rate\n",
    "\n",
    "    v: dict\n",
    "        contains previous W and b values, accumulated in a weighted fashion along with the gradients eg.\n",
    "        v[Wi] = beta1*v[Wi] + (1-beta1)*(gradient[dWi])\n",
    "\n",
    "    m: dict\n",
    "        contains previous W and b values, accumulated in a weighted fashion along with the gradients^2 eg.\n",
    "        v[Wi] = beta2*v[Wi] + (1-beta2)*(gradient[dWi]^2)\n",
    "\n",
    "    t: int\n",
    "        timestep for Adam\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    parameters: dict\n",
    "        updated NN parameters\n",
    "\n",
    "    v: dict\n",
    "        updated previous updates\n",
    "\n",
    "    m: dict\n",
    "        updated \"velocities\"\n",
    "\n",
    "    t: int\n",
    "        updated timestep\n",
    "\n",
    "    '''\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    beta1 = 0.9 #default\n",
    "    beta2 = 0.999 #default\n",
    "    epsilon = 1e-8 #for numerical stability\n",
    "\n",
    "    for l in range(1, L+1):\n",
    "        mdw = beta1*m[\"W\"+str(l)] + (1-beta1)*grads[\"dW\"+str(l)]\n",
    "        vdw = beta2*v[\"W\"+str(l)] + (1-beta2)*np.square(grads[\"dW\"+str(l)])\n",
    "        mw_hat = mdw/(1.0 - beta1**t)\n",
    "        vw_hat = vdw/(1.0 - beta2**t)\n",
    "\n",
    "        parameters[\"W\"+str(l)] = parameters[\"W\"+str(l)] - (learning_rate * mw_hat)/np.sqrt(vw_hat + epsilon)\n",
    "\n",
    "        mdb = beta1*m[\"b\"+str(l)] + (1-beta1)*grads[\"db\"+str(l)]\n",
    "        vdb = beta2*v[\"b\"+str(l)] + (1-beta2)*np.square(grads[\"db\"+str(l)])\n",
    "        mb_hat = mdb/(1.0 - beta1**t)\n",
    "        vb_hat = vdb/(1.0 - beta2**t)\n",
    "\n",
    "        parameters[\"b\"+str(l)] = parameters[\"b\"+str(l)] - (learning_rate * mb_hat)/np.sqrt(vb_hat + epsilon)\n",
    "\n",
    "        v[\"dW\"+str(l)] = vdw\n",
    "        m[\"dW\"+str(l)] = mdw\n",
    "        v[\"db\"+str(l)] = vdb\n",
    "        m[\"db\"+str(l)] = mdb\n",
    "\n",
    "    t = t + 1 # timestep\n",
    "    return parameters, v, m, t\n",
    "\n",
    "######## Enter your new optimiser updates function here #############\n",
    "#                                                                   #\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_predict(X_test, params, activation_f):\n",
    "    '''\n",
    "    forward propagate once and calculate labels\n",
    "\n",
    "    '''\n",
    "    output, _, _ = forward_propagate(X_test, params, activation_f)\n",
    "    predictions = np.argmax(output, axis=0)\n",
    "    return predictions\n",
    "\n",
    "def NN_evaluate(X_train, y_train, X_test, y_test, params, activation_f):\n",
    "    '''\n",
    "    print train,test accuracies and the classification report using sklearn\n",
    "\n",
    "    '''\n",
    "    train_predictions = NN_predict(X_train, params, activation_f)\n",
    "    test_predictions = NN_predict(X_test, params, activation_f)\n",
    "\n",
    "    print(\"Training accuracy = {} %\".format(round(accuracy_score(y_train, train_predictions) * 100, 3)))\n",
    "    print(\"Test accuracy = {} %\".format(round(accuracy_score(y_test, test_predictions) * 100, 3)))\n",
    "\n",
    "    print(\"Classification report for the test set:\\n\")\n",
    "    print(classification_report(y_test, test_predictions))\n",
    "\n",
    "    return train_predictions, test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def NN_fit(X_train, y_train_one_hot,X_val,y_val_one_hot, learning_rate = 0.001, activation_f = 'tanh', init_mode = 'xavier', \n",
    "                optimizer = 'adam', batch_size = 512, loss = 'categorical_crossentropy', epochs = 20, L2_lamb = 0,\n",
    "                layer_dims=[]):\n",
    "    \"\"\"This function is used to train the neural network on the dataset \n",
    "\n",
    "    X_train: numpy array\n",
    "        train dataset\n",
    "\n",
    "    y_train_one_hot: numpy array\n",
    "        train labels with one-hot encoding\n",
    "\n",
    "    learning_rate: float\n",
    "\n",
    "    activation_f: string\n",
    "        activation functions for all the layers except the last layer which is softmax\n",
    "\n",
    "    init_mode: string\n",
    "        initialization mode\n",
    "    \n",
    "    optimizer: string\n",
    "        optimization routine\n",
    "\n",
    "    bach_size: int\n",
    "        minibatch size\n",
    "\n",
    "    loss: string\n",
    "        loss function\n",
    "\n",
    "    epochs: int\n",
    "        number of epochs to be used\n",
    "\n",
    "    L2_lamb: float\n",
    "        lambda for L2 regularisation of weights\n",
    "\n",
    "    num_neurons: int\n",
    "        number of neurons in every hidden layer\n",
    "\n",
    "    num_hidden: \n",
    "        number of hidden layers\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    parameters: dict\n",
    "        weights and biases of the NN model\n",
    "\n",
    "    epoch_cost: list\n",
    "        training costs with every epoch\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    params, previous_updates = initialize_parameters(layer_dims, init_mode) # initialize the parameters and past updates matrices\n",
    "    \n",
    "    epoch_cost = []\n",
    "    validation_epoch_cost=[]\n",
    "    \n",
    "    count = 1\n",
    "    t = 1 # initialize timestep for Adam optimizer\n",
    "    v = previous_updates.copy()\n",
    "    m = previous_updates.copy()\n",
    "    params_look_ahead = params.copy() # initialization for nestorov\n",
    "    beta = 0.9\n",
    "    loss = 'categorical_crossentropy'  \n",
    "\n",
    "    while count<=epochs:\n",
    "        count = count + 1 # increment the number of epochs\n",
    "\n",
    "        for i in range(0, X_train.shape[1], batch_size):\n",
    "            batch_count = batch_size\n",
    "\n",
    "            if i + batch_size > X_train.shape[1]: # the last mini-batch might contain fewer than \"batch_size\" examples\n",
    "                batch_count = X_train.shape[1] - i + 1\n",
    "            \n",
    "            #process all nesterov accelerated optimisers\n",
    "\n",
    "            #NAG\n",
    "            if optimizer == 'nesterov':\n",
    "                L = len(params)//2\n",
    "\n",
    "                #look ahead logic\n",
    "                for l in range(1, L+1):\n",
    "                    params_look_ahead[\"W\"+str(l)] = params[\"W\"+str(l)] - beta*previous_updates[\"W\"+str(l)]\n",
    "                    params_look_ahead[\"b\"+str(l)] = params[\"b\"+str(l)] - beta*previous_updates[\"b\"+str(l)]\n",
    "                    \n",
    "                output,A,Z = forward_propagate(X_train[:,i:i+batch_size],params_look_ahead,activation_f)\n",
    "                gradients = backprop(output,y_train_one_hot[:,i:i+batch_size],A,Z,params_look_ahead,activation_f, batch_count, loss, L2_lamb)\n",
    "\n",
    "                #call momentum\n",
    "                params,previous_updates = update_parameters_momentum(params, gradients, learning_rate, beta, previous_updates)\n",
    "\n",
    "            #nadam\n",
    "            elif optimizer=='nadam':\n",
    "                L = len(params)//2\n",
    "\n",
    "                #look ahead logic\n",
    "                for l in range(1, L+1):\n",
    "                    params_look_ahead[\"W\"+str(l)] = params[\"W\"+str(l)] - beta*previous_updates[\"W\"+str(l)]\n",
    "                    params_look_ahead[\"b\"+str(l)] = params[\"b\"+str(l)] - beta*previous_updates[\"b\"+str(l)]\n",
    "\n",
    "                output,A,Z = forward_propagate(X_train[:,i:i+batch_size],params_look_ahead,activation_f)\n",
    "                gradients = backprop(output,y_train_one_hot[:,i:i+batch_size],A,Z,params_look_ahead,activation_f, batch_count, loss, L2_lamb)\n",
    "\n",
    "                #call adam\n",
    "                params, v, m, t = update_parameters_adam(params, gradients, learning_rate, v, m, t)\n",
    "\n",
    "            #custom\n",
    "            elif optimizer == 'insert your nesterov accelerated optimiser here':\n",
    "                #insert your optimiser here if it is nesterov accelerated\n",
    "\n",
    "                #insert lookahead logic here\n",
    "\n",
    "                #for weight updates, call that optimiser's weight update code here\n",
    "\n",
    "                pass\n",
    "\n",
    "            #process all other optimisers\n",
    "\n",
    "            else:\n",
    "                output,A,Z = forward_propagate(X_train[:,i:i+batch_size],params,activation_f)\n",
    "                gradients = backprop(output,y_train_one_hot[:,i:i+batch_size],A,Z,params,activation_f, batch_count, loss, L2_lamb)\n",
    "\n",
    "                if optimizer == 'sgd':\n",
    "                    params = update_params_sgd(params,gradients,learning_rate)\n",
    "                elif optimizer == 'momentum':\n",
    "                    params,previous_updates = update_parameters_momentum(params, gradients, learning_rate, beta, previous_updates)\n",
    "                elif optimizer == 'RMSprop':\n",
    "                    params,previous_updates = update_parameters_RMSprop(params, gradients, learning_rate, beta, previous_updates)\n",
    "                elif optimizer == 'adam':\n",
    "                    params, v, m, t = update_parameters_adam(params, gradients, learning_rate, v, m, t)\n",
    "\n",
    "                #custom\n",
    "                elif optimizer == 'insert your optimiser here':\n",
    "                    #insert your optimiser update routine only if it does not have nesterov \n",
    "                    pass\n",
    "\n",
    "                    \n",
    "        # Mean loss for the full training set\n",
    "        full_output, _, _ = forward_propagate(X_train, params, activation_f)\n",
    "        cost = compute_multiclass_loss(y_train_one_hot, full_output, M, loss, L2_lamb, params)\n",
    "        epoch_cost.append(cost)\n",
    "        \n",
    "        # Mean loss for the validation set\n",
    "        out, _, _ = forward_propagate(X_val, params, activation_f)\n",
    "        val_cost = compute_multiclass_loss(y_val_one_hot, out, Mval, loss, L2_lamb, params)\n",
    "        validation_epoch_cost.append(val_cost)\n",
    "\n",
    "        if (count % 2 == 0):\n",
    "            print(\"Epoch number: \", count, \"\\tTraining cost:\", cost)\n",
    "\n",
    "\n",
    "    return params, epoch_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "ACTIVATION = \"sigmoid\"\n",
    "INITIALIZER = \"random\"\n",
    "OPTIMIZER = \"sgd\"\n",
    "BATCH_SIZE = 200\n",
    "EPOCHS = 30\n",
    "L2_lambda = 0.0005\n",
    "LAYER_DIMS = [784,64,32,10]\n",
    "\n",
    "LOSS = 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'W1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_151640/2805691057.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m learned_parameters, epoch_cost = NN_fit(X_train, y_train_one_hot,\n\u001b[0m\u001b[1;32m      2\u001b[0m                             \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val_one_hot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                             \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                             \u001b[0mactivation_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mACTIVATION\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                             \u001b[0minit_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mINITIALIZER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_151640/2708594851.py\u001b[0m in \u001b[0;36mNN_fit\u001b[0;34m(X_train, y_train_one_hot, X_val, y_val_one_hot, learning_rate, activation_f, init_mode, optimizer, batch_size, loss, epochs, L2_lamb, layer_dims)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_propagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m                 \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_one_hot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2_lamb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_151640/3024098191.py\u001b[0m in \u001b[0;36mforward_propagate\u001b[0;34m(X, params, activation_f)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'W1'"
     ]
    }
   ],
   "source": [
    "learned_parameters, epoch_cost = NN_fit(X_train, y_train_one_hot,\n",
    "                            X_val,y_val_one_hot,\n",
    "                            learning_rate=LEARNING_RATE,\n",
    "                            activation_f = ACTIVATION,\n",
    "                            init_mode = INITIALIZER,\n",
    "                            optimizer = OPTIMIZER,\n",
    "                            batch_size = BATCH_SIZE,\n",
    "                            loss = LOSS,\n",
    "                            epochs = EPOCHS,\n",
    "                            L2_lamb = L2_lambda,\n",
    "                            layer_dims = LAYER_DIMS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd1bac42fb879949742610e05f4168fcf894caa2bd3b95e8fef3e645755aa079"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
